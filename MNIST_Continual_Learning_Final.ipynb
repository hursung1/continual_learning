{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_Continual_Learning_Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxSnkStJoYDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXtTo6L2TeQG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PermutedMNISTDataLoader(torchvision.datasets.MNIST):\n",
        "    \n",
        "    def __init__(self, source='./mnist_data', train = True, shuffle_seed = None):\n",
        "        super(PermutedMNISTDataLoader, self).__init__(source, train, download=True)\n",
        "        \n",
        "        self.train = train\n",
        "        if self.train:\n",
        "            self.permuted_train_data = torch.stack(\n",
        "                [img.type(dtype=torch.float32).view(-1)[shuffle_seed] / 255.0\n",
        "                    for img in self.train_data])\n",
        "        else:\n",
        "            self.permuted_test_data = torch.stack(\n",
        "                [img.type(dtype=torch.float32).view(-1)[shuffle_seed] / 255.0\n",
        "                    for img in self.test_data])\n",
        "            \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        if self.train:\n",
        "            input, label = self.permuted_train_data[index], self.train_labels[index]\n",
        "        else:\n",
        "            input, label = self.permuted_test_data[index], self.test_labels[index]\n",
        "        \n",
        "        return input, label\n",
        "\n",
        "    def sample(self, size):\n",
        "        return [img for img in self.permuted_train_data[random.sample(range(len(self), size))]]\n",
        "    \n",
        "    '''\n",
        "    def __len__(self):\n",
        "        if self.train:\n",
        "            return self.train_data.size()\n",
        "        else:\n",
        "            return self.test_data.size()\n",
        "    '''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEUfd3K_oiYV",
        "colab_type": "code",
        "outputId": "89702acb-0af5-4656-a2dc-2701eb39b2e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "batch_size = 64\n",
        "num_tasks = 10\n",
        "\n",
        "def permute_mnist():\n",
        "    train_loader = {}\n",
        "    test_loader = {}\n",
        "    \n",
        "    for i in range(num_tasks):\n",
        "        shuffle_seed = np.arange(28*28)\n",
        "        np.random.shuffle(shuffle_seed)\n",
        "        train_loader[i] = torch.utils.data.DataLoader(\n",
        "            PermutedMNISTDataLoader(train=True, shuffle_seed=shuffle_seed),\n",
        "                batch_size=batch_size)\n",
        "        \n",
        "        test_loader[i] = torch.utils.data.DataLoader(\n",
        "            PermutedMNISTDataLoader(train=False, shuffle_seed=shuffle_seed),\n",
        "                batch_size=batch_size)\n",
        "    \n",
        "    return train_loader, test_loader\n",
        "\n",
        "train_loader, test_loader = permute_mnist()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./mnist_data/PermutedMNISTDataLoader/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:02, 3609153.60it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./mnist_data/PermutedMNISTDataLoader/raw/train-images-idx3-ubyte.gz to ./mnist_data/PermutedMNISTDataLoader/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./mnist_data/PermutedMNISTDataLoader/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 58503.21it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./mnist_data/PermutedMNISTDataLoader/raw/train-labels-idx1-ubyte.gz to ./mnist_data/PermutedMNISTDataLoader/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./mnist_data/PermutedMNISTDataLoader/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:01, 973737.59it/s]                             \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./mnist_data/PermutedMNISTDataLoader/raw/t10k-images-idx3-ubyte.gz to ./mnist_data/PermutedMNISTDataLoader/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./mnist_data/PermutedMNISTDataLoader/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 21788.05it/s]            \n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:53: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./mnist_data/PermutedMNISTDataLoader/raw/t10k-labels-idx1-ubyte.gz to ./mnist_data/PermutedMNISTDataLoader/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:58: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M6_A9nTJYDi",
        "colab_type": "text"
      },
      "source": [
        "### Define Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfTLE0m3opep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        # Always start with inheriting torch.nn.Module\n",
        "        # Ancestor class of all Neural Net module\n",
        "        super(NeuralNet, self).__init__()\n",
        "        # Linear: linear transformation\n",
        "        fc1 = nn.Linear(28*28, 400)\n",
        "        fc2 = nn.Linear(400, 400)\n",
        "        fc3 = nn.Linear(400, 10)\n",
        "  \n",
        "        \n",
        "        self.fc_module = nn.Sequential(\n",
        "            fc1,\n",
        "            nn.ReLU(),\n",
        "            fc2,\n",
        "            nn.ReLU(),\n",
        "            fc3\n",
        "        )\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            self.fc_module = self.fc_module.cuda()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc_module(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkX7OCUYJO5R",
        "colab_type": "text"
      },
      "source": [
        "### Get Fisher Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7OWWTJV3KpZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fisher(net, data_loader, task):\n",
        "    fisher_mat = []       \n",
        "    for i in range(task):\n",
        "        for n, data in enumerate(data_loader[i]):\n",
        "            if(n == task-i-1):\n",
        "                data_ = data[0]\n",
        "                break\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            data_ = data_.cuda()\n",
        "\n",
        "        params = {n : p for n, p in net.named_parameters() if p.requires_grad}\n",
        "        fisher_mat_per_task = {}\n",
        "        for n, p in deepcopy(params).items():\n",
        "            p.data.zero_()\n",
        "            fisher_mat_per_task[n] = p.data\n",
        "\n",
        "        net.eval()\n",
        "        for data in data_:\n",
        "            net.zero_grad()\n",
        "            output = net(data).view(1, -1)\n",
        "            pred = output.max(1)[1].view(-1)\n",
        "            loss = F.nll_loss(F.log_softmax(output, dim=1), pred)\n",
        "            loss.backward()\n",
        "\n",
        "            for n, p in net.named_parameters():\n",
        "                fisher_mat_per_task[n].data += p.grad.data ** 2 / len(data_)\n",
        "\n",
        "        fisher_mat.append({n : p for n, p in fisher_mat_per_task.items()})\n",
        "    return fisher_mat\n",
        "\n",
        "############################################################################                \n",
        "    '''\n",
        "    params = {n : p for n, p in net.named_parameters() if p.requires_grad}\n",
        "    fisher_mat = {}\n",
        "    for n, p in deepcopy(params).items():\n",
        "        p.data.zero_()\n",
        "        fisher_mat[n] = p.data\n",
        "\n",
        "    net.eval()\n",
        "    for data in input:\n",
        "        net.zero_grad()\n",
        "        output = net(data).view(1, -1)\n",
        "        pred = output.max(1)[1].view(-1)\n",
        "        loss = F.nll_loss(F.log_softmax(output, dim=1), pred)\n",
        "        loss.backward()\n",
        "\n",
        "        for n, p in net.named_parameters():\n",
        "            fisher_mat[n].data += p.grad.data ** 2 / len(input)\n",
        "\n",
        "    fisher_mat = {n : p for n, p in fisher_mat.items()}\n",
        "    #print(\"Time: %.3f\" %(time.time() - start_time))\n",
        "    return fisher_mat\n",
        "    '''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tilDKYw6JT8v",
        "colab_type": "text"
      },
      "source": [
        "### Learning Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dV5puTlos0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Learning_Function(net, optimizer, num_tasks, reg_coef, learn_mode = 0):\n",
        "    if learn_mode > 2 or learn_mode < 0:\n",
        "        print(\"Learn mode Error\\nplain: 0\\tpenalty with L2 distance: 1\\tpenalty with ewc: 2\")\n",
        "        return False\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    num_epochs = 20\n",
        "    sample_size = 100\n",
        "    acc = {}\n",
        "    params_per_tasks = []\n",
        "\n",
        "    print(\"Task\\tEpoch\")\n",
        "    for task in range(num_tasks):\n",
        "        running_loss = 0.0\n",
        "        \n",
        "        # Get Fisher Matrix\n",
        "        if len(params_per_tasks) != 0 and learn_mode == 2:\n",
        "            fisher_mat = fisher(net, train_loader, task)\n",
        "            \n",
        "        # Train for each task\n",
        "        for epoch in range(num_epochs):\n",
        "            for i, data in enumerate(train_loader[task]):\n",
        "                inputs, labels = data\n",
        "\n",
        "                if torch.cuda.is_available():\n",
        "                    inputs = inputs.cuda()\n",
        "                    labels = labels.cuda()\n",
        "\n",
        "                # gradient initiallize\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Compute forward-propagation\n",
        "                outputs = net(inputs)\n",
        "\n",
        "                # Compute Loss\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Compute Loss & L2 distance\n",
        "                if learn_mode != 0:\n",
        "                    reg = 0\n",
        "                    ind = 0\n",
        "                    for params_past in params_per_tasks:\n",
        "                        for n, p in net.named_parameters():\n",
        "                            if torch.cuda.is_available():\n",
        "                                params_past[n] = params_past[n].cuda()\n",
        "                                \n",
        "                            penalty = (params_past[n] - p)**2\n",
        "                            # EWC: multiply fisher matrix\n",
        "                            if learn_mode == 2:\n",
        "                                penalty = fisher_mat[ind][n] * penalty\n",
        "\n",
        "                            reg += torch.sum(penalty)\n",
        "                        ind += 1\n",
        "                    loss = loss + (reg_coef / 2) * reg\n",
        "\n",
        "                #Do Back-propagation\n",
        "                loss.backward()\n",
        "                #Weight update\n",
        "                optimizer.step()\n",
        "\n",
        "                #cumulate loss\n",
        "                running_loss += loss.data.item()\n",
        "\n",
        "            if epoch % 5 == 4:\n",
        "                print('[%d\\t%d] AVG. loss: %.3f' % (task+1, epoch + 1, running_loss/(i*5)))\n",
        "                running_loss = 0.0\n",
        "\n",
        "        # Save parameters to use at next iteration: used to cal. penalty term      \n",
        "        if learn_mode != 0:\n",
        "            tp = {n : p for n, p in net.named_parameters() if p.requires_grad}\n",
        "            params = {}\n",
        "            for n, p in deepcopy(tp).items():\n",
        "                params[n] = p.data\n",
        "            params_per_tasks.append(params)\n",
        "\n",
        "            \n",
        "        # Test for each task after learning a task.\n",
        "        each_task_acc = []\n",
        "        for j in range(task+1):\n",
        "            total = 0\n",
        "            correct = 0\n",
        "            for i, data in enumerate(test_loader[j]):\n",
        "                inputs, labels = data\n",
        "                if torch.cuda.is_available():\n",
        "                    inputs = inputs.cuda()\n",
        "                    labels = labels.cuda()\n",
        "\n",
        "                # forward propagation\n",
        "                outputs = net(inputs)\n",
        "\n",
        "                # torch.max: returns maximum value of a tensor\n",
        "                _, predicted = torch.max(outputs.data, dim=1)\n",
        "                total += labels.size(0)\n",
        "                \n",
        "                # Estimate accuracy of model\n",
        "                correct += (predicted == labels).sum()\n",
        "                                \n",
        "            each_task_acc.append(100 * correct.cpu().numpy() / total)\n",
        "            \n",
        "        print(each_task_acc)\n",
        "        each_task_acc = np.asarray(each_task_acc)\n",
        "        print(each_task_acc)\n",
        "        print('Average accuracy after training task %d: %d %%' % (task+1, np.mean(each_task_acc)))\n",
        "        acc[task] = np.mean(each_task_acc)\n",
        "            # For each input data, print the accuracy of the model\n",
        "            #print('Accuracy of the network on the test images %d for task %d after training task %d: %d %%' \n",
        "            #                % (i, j, task+1, 100 * correct / total))\n",
        "            \n",
        "    return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTYvapqh2kzS",
        "colab_type": "text"
      },
      "source": [
        "### Plain SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEn6wMQn1eKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Plain_SGD_Learning(learning_rate, num_task):\n",
        "    net = NeuralNet()\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"Use GPU\")\n",
        "        net.cuda()\n",
        "        \n",
        "    optimizer = optim.SGD(net.parameters(), lr=learning_rate)    \n",
        "    return Learning_Function(net, optimizer, num_task, reg_coef=1, learn_mode=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm5QWxH82rRW",
        "colab_type": "text"
      },
      "source": [
        "### Continual Learning with L2 distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_3l5Jps1hpB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L2_Learning(learning_rate, num_task):\n",
        "    learning_acc = {}\n",
        "    reg_coef_list = [300,1000,3000,10000]\n",
        "    for reg_coef in reg_coef_list:\n",
        "        net = NeuralNet()\n",
        "        if torch.cuda.is_available():\n",
        "            print(\"Use GPU\")\n",
        "            net.cuda()\n",
        "            \n",
        "        optimizer = optim.SGD(net.parameters(), lr=learning_rate)    \n",
        "        print(\"Penalty term coefficient: \", reg_coef)\n",
        "        learning_acc[reg_coef] = Continual_Learning(net, optimizer, num_task, reg_coef, learn_mode=2)\n",
        "\n",
        "    return learning_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjRo5YUR2wFz",
        "colab_type": "text"
      },
      "source": [
        "### Continual Learning with Elastic Weight Consolidation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM9tCwy31h6Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def EWC_Learning(learning_rate, num_task):\n",
        "    learning_acc = {}\n",
        "    reg_coef_list = [3, 10, 30, 100, 300, 1000, 3000, 10000]\n",
        "    for reg_coef in reg_coef_list:\n",
        "        net = NeuralNet()\n",
        "        if torch.cuda.is_available():\n",
        "            print(\"Use GPU\")\n",
        "            net.cuda()\n",
        "            \n",
        "        optimizer = optim.SGD(net.parameters(), lr=learning_rate)    \n",
        "        print(\"Penalty term coefficient: \", reg_coef)\n",
        "        learning_acc[reg_coef] = Continual_Learning(net, optimizer, num_task, reg_coef, learn_mode=2)\n",
        "\n",
        "    return learning_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1oVDzZEqL92",
        "colab_type": "code",
        "outputId": "572d4b03-219d-4c55-cffd-920b0b9e63d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "learning_rate = 1e-3\n",
        "num_task = 10\n",
        "\n",
        "SGD_acc = Plain_SGD_Learning(learning_rate, num_task)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Use GPU\n",
            "Task\tEpoch\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:43: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1\t5] AVG. loss: 2.111\n",
            "[1\t10] AVG. loss: 1.069\n",
            "[1\t15] AVG. loss: 0.569\n",
            "[1\t20] AVG. loss: 0.443\n",
            "[89.14]\n",
            "[89.14]\n",
            "Average accuracy after training task 1: 89 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:48: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2\t5] AVG. loss: 0.786\n",
            "[2\t10] AVG. loss: 0.436\n",
            "[2\t15] AVG. loss: 0.373\n",
            "[2\t20] AVG. loss: 0.340\n",
            "[86.53, 90.9]\n",
            "[86.53 90.9 ]\n",
            "Average accuracy after training task 2: 88 %\n",
            "[3\t5] AVG. loss: 0.643\n",
            "[3\t10] AVG. loss: 0.388\n",
            "[3\t15] AVG. loss: 0.342\n",
            "[3\t20] AVG. loss: 0.314\n",
            "[79.7, 87.65, 91.49]\n",
            "[79.7  87.65 91.49]\n",
            "Average accuracy after training task 3: 86 %\n",
            "[4\t5] AVG. loss: 0.605\n",
            "[4\t10] AVG. loss: 0.369\n",
            "[4\t15] AVG. loss: 0.326\n",
            "[4\t20] AVG. loss: 0.300\n",
            "[77.31, 82.56, 89.13, 91.94]\n",
            "[77.31 82.56 89.13 91.94]\n",
            "Average accuracy after training task 4: 85 %\n",
            "[5\t5] AVG. loss: 0.596\n",
            "[5\t10] AVG. loss: 0.349\n",
            "[5\t15] AVG. loss: 0.306\n",
            "[5\t20] AVG. loss: 0.280\n",
            "[75.08, 76.49, 82.57, 86.03, 92.41]\n",
            "[75.08 76.49 82.57 86.03 92.41]\n",
            "Average accuracy after training task 5: 82 %\n",
            "[6\t5] AVG. loss: 0.574\n",
            "[6\t10] AVG. loss: 0.339\n",
            "[6\t15] AVG. loss: 0.296\n",
            "[6\t20] AVG. loss: 0.269\n",
            "[72.97, 73.89, 77.2, 78.82, 89.23, 92.71]\n",
            "[72.97 73.89 77.2  78.82 89.23 92.71]\n",
            "Average accuracy after training task 6: 80 %\n",
            "[7\t5] AVG. loss: 0.554\n",
            "[7\t10] AVG. loss: 0.329\n",
            "[7\t15] AVG. loss: 0.287\n",
            "[7\t20] AVG. loss: 0.261\n",
            "[69.74, 68.26, 77.07, 78.25, 86.88, 85.07, 92.81]\n",
            "[69.74 68.26 77.07 78.25 86.88 85.07 92.81]\n",
            "Average accuracy after training task 7: 79 %\n",
            "[8\t5] AVG. loss: 0.567\n",
            "[8\t10] AVG. loss: 0.328\n",
            "[8\t15] AVG. loss: 0.285\n",
            "[8\t20] AVG. loss: 0.258\n",
            "[62.94, 64.61, 73.99, 75.52, 79.33, 75.9, 89.17, 93.03]\n",
            "[62.94 64.61 73.99 75.52 79.33 75.9  89.17 93.03]\n",
            "Average accuracy after training task 8: 76 %\n",
            "[9\t5] AVG. loss: 0.563\n",
            "[9\t10] AVG. loss: 0.317\n",
            "[9\t15] AVG. loss: 0.274\n",
            "[9\t20] AVG. loss: 0.247\n",
            "[65.66, 56.86, 60.18, 70.11, 71.81, 74.15, 85.46, 89.73, 93.25]\n",
            "[65.66 56.86 60.18 70.11 71.81 74.15 85.46 89.73 93.25]\n",
            "Average accuracy after training task 9: 74 %\n",
            "[10\t5] AVG. loss: 0.540\n",
            "[10\t10] AVG. loss: 0.307\n",
            "[10\t15] AVG. loss: 0.264\n",
            "[10\t20] AVG. loss: 0.238\n",
            "[58.38, 62.17, 55.93, 65.52, 66.52, 65.17, 82.45, 86.42, 89.43, 93.31]\n",
            "[58.38 62.17 55.93 65.52 66.52 65.17 82.45 86.42 89.43 93.31]\n",
            "Average accuracy after training task 10: 72 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnVxh4mi5VPD",
        "colab_type": "code",
        "outputId": "16418d28-7712-46b6-9f93-9abafdbd5bf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "x, y = list(SGD_acc.keys()), list(SGD_acc.values())\n",
        "plt.xlabel(\"# of tasks\")\n",
        "plt.ylabel(\"Avg. acc\")\n",
        "plt.title(\"Average Accuracies: Plain SGD\")\n",
        "plt.plot(x, y)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb-0sgFDbQPe",
        "colab_type": "code",
        "outputId": "2f2316e5-d33a-41d1-fc46-a471f8934dde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EWC_acc = EWC_Learning(learning_rate, num_task)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Use GPU\n",
            "Penalty term coefficient:  3\n",
            "Task\tEpoch\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:43: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1\t5] AVG. loss: 2.115\n",
            "[1\t10] AVG. loss: 1.063\n",
            "[1\t15] AVG. loss: 0.567\n",
            "[1\t20] AVG. loss: 0.443\n",
            "[89.25]\n",
            "[89.25]\n",
            "Average accuracy after training task 1: 89 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:48: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2\t5] AVG. loss: 0.790\n",
            "[2\t10] AVG. loss: 0.443\n",
            "[2\t15] AVG. loss: 0.383\n",
            "[2\t20] AVG. loss: 0.351\n",
            "[86.61, 90.79]\n",
            "[86.61 90.79]\n",
            "Average accuracy after training task 2: 88 %\n",
            "[3\t5] AVG. loss: 0.676\n",
            "[3\t10] AVG. loss: 0.412\n",
            "[3\t15] AVG. loss: 0.367\n",
            "[3\t20] AVG. loss: 0.341\n",
            "[79.93, 87.84, 91.48]\n",
            "[79.93 87.84 91.48]\n",
            "Average accuracy after training task 3: 86 %\n",
            "[4\t5] AVG. loss: 0.646\n",
            "[4\t10] AVG. loss: 0.407\n",
            "[4\t15] AVG. loss: 0.365\n",
            "[4\t20] AVG. loss: 0.341\n",
            "[79.0, 82.69, 88.8, 91.66]\n",
            "[79.   82.69 88.8  91.66]\n",
            "Average accuracy after training task 4: 85 %\n",
            "[5\t5] AVG. loss: 0.670\n",
            "[5\t10] AVG. loss: 0.415\n",
            "[5\t15] AVG. loss: 0.374\n",
            "[5\t20] AVG. loss: 0.350\n",
            "[81.17, 79.6, 82.62, 86.36, 91.78]\n",
            "[81.17 79.6  82.62 86.36 91.78]\n",
            "Average accuracy after training task 5: 84 %\n",
            "[6\t5] AVG. loss: 0.677\n",
            "[6\t10] AVG. loss: 0.433\n",
            "[6\t15] AVG. loss: 0.391\n",
            "[6\t20] AVG. loss: 0.366\n",
            "[77.15, 79.54, 80.79, 82.28, 87.74, 92.0]\n",
            "[77.15 79.54 80.79 82.28 87.74 92.  ]\n",
            "Average accuracy after training task 6: 83 %\n",
            "[7\t5] AVG. loss: 0.663\n",
            "[7\t10] AVG. loss: 0.433\n",
            "[7\t15] AVG. loss: 0.393\n",
            "[7\t20] AVG. loss: 0.369\n",
            "[74.34, 78.74, 80.48, 79.84, 84.12, 83.67, 92.13]\n",
            "[74.34 78.74 80.48 79.84 84.12 83.67 92.13]\n",
            "Average accuracy after training task 7: 81 %\n",
            "[8\t5] AVG. loss: 0.723\n",
            "[8\t10] AVG. loss: 0.471\n",
            "[8\t15] AVG. loss: 0.428\n",
            "[8\t20] AVG. loss: 0.404\n",
            "[72.66, 78.82, 81.42, 78.98, 78.31, 74.34, 88.61, 92.08]\n",
            "[72.66 78.82 81.42 78.98 78.31 74.34 88.61 92.08]\n",
            "Average accuracy after training task 8: 80 %\n",
            "[9\t5] AVG. loss: 0.772\n",
            "[9\t10] AVG. loss: 0.505\n",
            "[9\t15] AVG. loss: 0.461\n",
            "[9\t20] AVG. loss: 0.435\n",
            "[78.1, 77.88, 73.12, 74.15, 76.99, 72.7, 80.49, 86.79, 92.24]\n",
            "[78.1  77.88 73.12 74.15 76.99 72.7  80.49 86.79 92.24]\n",
            "Average accuracy after training task 9: 79 %\n",
            "[10\t5] AVG. loss: 0.771\n",
            "[10\t10] AVG. loss: 0.527\n",
            "[10\t15] AVG. loss: 0.483\n",
            "[10\t20] AVG. loss: 0.456\n",
            "[76.27, 80.31, 76.88, 73.79, 74.6, 67.67, 75.0, 75.55, 84.73, 92.01]\n",
            "[76.27 80.31 76.88 73.79 74.6  67.67 75.   75.55 84.73 92.01]\n",
            "Average accuracy after training task 10: 77 %\n",
            "Use GPU\n",
            "Penalty term coefficient:  10\n",
            "Task\tEpoch\n",
            "[1\t5] AVG. loss: 2.100\n",
            "[1\t10] AVG. loss: 1.040\n",
            "[1\t15] AVG. loss: 0.561\n",
            "[1\t20] AVG. loss: 0.439\n",
            "[89.23]\n",
            "[89.23]\n",
            "Average accuracy after training task 1: 89 %\n",
            "[2\t5] AVG. loss: 0.803\n",
            "[2\t10] AVG. loss: 0.448\n",
            "[2\t15] AVG. loss: 0.389\n",
            "[2\t20] AVG. loss: 0.358\n",
            "[86.01, 90.52]\n",
            "[86.01 90.52]\n",
            "Average accuracy after training task 2: 88 %\n",
            "[3\t5] AVG. loss: 0.704\n",
            "[3\t10] AVG. loss: 0.434\n",
            "[3\t15] AVG. loss: 0.387\n",
            "[3\t20] AVG. loss: 0.361\n",
            "[80.21, 87.58, 91.37]\n",
            "[80.21 87.58 91.37]\n",
            "Average accuracy after training task 3: 86 %\n",
            "[4\t5] AVG. loss: 0.712\n",
            "[4\t10] AVG. loss: 0.459\n",
            "[4\t15] AVG. loss: 0.414\n",
            "[4\t20] AVG. loss: 0.389\n",
            "[82.29, 82.96, 88.86, 91.3]\n",
            "[82.29 82.96 88.86 91.3 ]\n",
            "Average accuracy after training task 4: 86 %\n",
            "[5\t5] AVG. loss: 0.743\n",
            "[5\t10] AVG. loss: 0.478\n",
            "[5\t15] AVG. loss: 0.434\n",
            "[5\t20] AVG. loss: 0.408\n",
            "[83.39, 81.09, 82.24, 85.72, 91.42]\n",
            "[83.39 81.09 82.24 85.72 91.42]\n",
            "Average accuracy after training task 5: 84 %\n",
            "[6\t5] AVG. loss: 0.762\n",
            "[6\t10] AVG. loss: 0.506\n",
            "[6\t15] AVG. loss: 0.462\n",
            "[6\t20] AVG. loss: 0.435\n",
            "[79.29, 83.51, 82.59, 81.11, 85.61, 91.58]\n",
            "[79.29 83.51 82.59 81.11 85.61 91.58]\n",
            "Average accuracy after training task 6: 83 %\n",
            "[7\t5] AVG. loss: 0.760\n",
            "[7\t10] AVG. loss: 0.518\n",
            "[7\t15] AVG. loss: 0.476\n",
            "[7\t20] AVG. loss: 0.451\n",
            "[78.03, 83.39, 81.76, 80.23, 76.11, 77.24, 91.52]\n",
            "[78.03 83.39 81.76 80.23 76.11 77.24 91.52]\n",
            "Average accuracy after training task 7: 81 %\n",
            "[8\t5] AVG. loss: 0.863\n",
            "[8\t10] AVG. loss: 0.603\n",
            "[8\t15] AVG. loss: 0.555\n",
            "[8\t20] AVG. loss: 0.527\n",
            "[78.48, 82.23, 83.6, 79.29, 79.08, 69.48, 82.29, 91.54]\n",
            "[78.48 82.23 83.6  79.29 79.08 69.48 82.29 91.54]\n",
            "Average accuracy after training task 8: 80 %\n",
            "[9\t5] AVG. loss: 0.936\n",
            "[9\t10] AVG. loss: 0.662\n",
            "[9\t15] AVG. loss: 0.613\n",
            "[9\t20] AVG. loss: 0.584\n",
            "[81.69, 82.89, 82.61, 77.64, 80.0, 72.98, 71.91, 80.42, 91.43]\n",
            "[81.69 82.89 82.61 77.64 80.   72.98 71.91 80.42 91.43]\n",
            "Average accuracy after training task 9: 80 %\n",
            "[10\t5] AVG. loss: 0.942\n",
            "[10\t10] AVG. loss: 0.693\n",
            "[10\t15] AVG. loss: 0.644\n",
            "[10\t20] AVG. loss: 0.616\n",
            "[79.36, 79.2, 79.09, 75.68, 74.67, 65.83, 73.0, 63.81, 70.86, 91.31]\n",
            "[79.36 79.2  79.09 75.68 74.67 65.83 73.   63.81 70.86 91.31]\n",
            "Average accuracy after training task 10: 75 %\n",
            "Use GPU\n",
            "Penalty term coefficient:  30\n",
            "Task\tEpoch\n",
            "[1\t5] AVG. loss: 2.129\n",
            "[1\t10] AVG. loss: 1.094\n",
            "[1\t15] AVG. loss: 0.570\n",
            "[1\t20] AVG. loss: 0.442\n",
            "[88.97]\n",
            "[88.97]\n",
            "Average accuracy after training task 1: 88 %\n",
            "[2\t5] AVG. loss: 0.801\n",
            "[2\t10] AVG. loss: 0.462\n",
            "[2\t15] AVG. loss: 0.405\n",
            "[2\t20] AVG. loss: 0.376\n",
            "[87.27, 90.33]\n",
            "[87.27 90.33]\n",
            "Average accuracy after training task 2: 88 %\n",
            "[3\t5] AVG. loss: 0.744\n",
            "[3\t10] AVG. loss: 0.470\n",
            "[3\t15] AVG. loss: 0.424\n",
            "[3\t20] AVG. loss: 0.398\n",
            "[83.01, 87.49, 90.96]\n",
            "[83.01 87.49 90.96]\n",
            "Average accuracy after training task 3: 87 %\n",
            "[4\t5] AVG. loss: 0.754\n",
            "[4\t10] AVG. loss: 0.505\n",
            "[4\t15] AVG. loss: 0.460\n",
            "[4\t20] AVG. loss: 0.435\n",
            "[84.05, 84.61, 87.42, 91.06]\n",
            "[84.05 84.61 87.42 91.06]\n",
            "Average accuracy after training task 4: 86 %\n",
            "[5\t5] AVG. loss: 0.837\n",
            "[5\t10] AVG. loss: 0.568\n",
            "[5\t15] AVG. loss: 0.519\n",
            "[5\t20] AVG. loss: 0.492\n",
            "[84.78, 84.92, 80.63, 82.18, 90.75]\n",
            "[84.78 84.92 80.63 82.18 90.75]\n",
            "Average accuracy after training task 5: 84 %\n",
            "[6\t5] AVG. loss: 0.902\n",
            "[6\t10] AVG. loss: 0.639\n",
            "[6\t15] AVG. loss: 0.588\n",
            "[6\t20] AVG. loss: 0.560\n",
            "[84.0, 86.58, 83.24, 80.15, 77.65, 90.89]\n",
            "[84.   86.58 83.24 80.15 77.65 90.89]\n",
            "Average accuracy after training task 6: 83 %\n",
            "[7\t5] AVG. loss: 0.900\n",
            "[7\t10] AVG. loss: 0.648\n",
            "[7\t15] AVG. loss: 0.601\n",
            "[7\t20] AVG. loss: 0.574\n",
            "[82.59, 86.13, 82.88, 82.62, 74.81, 62.34, 90.61]\n",
            "[82.59 86.13 82.88 82.62 74.81 62.34 90.61]\n",
            "Average accuracy after training task 7: 80 %\n",
            "[8\t5] AVG. loss: 1.049\n",
            "[8\t10] AVG. loss: 0.780\n",
            "[8\t15] AVG. loss: 0.728\n",
            "[8\t20] AVG. loss: 0.700\n",
            "[83.35, 85.14, 84.04, 80.63, 80.61, 70.15, 70.63, 90.2]\n",
            "[83.35 85.14 84.04 80.63 80.61 70.15 70.63 90.2 ]\n",
            "Average accuracy after training task 8: 80 %\n",
            "[9\t5] AVG. loss: 1.198\n",
            "[9\t10] AVG. loss: 0.905\n",
            "[9\t15] AVG. loss: 0.853\n",
            "[9\t20] AVG. loss: 0.825\n",
            "[85.74, 84.95, 82.92, 78.83, 79.19, 70.22, 68.29, 61.27, 90.16]\n",
            "[85.74 84.95 82.92 78.83 79.19 70.22 68.29 61.27 90.16]\n",
            "Average accuracy after training task 9: 77 %\n",
            "[10\t5] AVG. loss: 1.324\n",
            "[10\t10] AVG. loss: 1.051\n",
            "[10\t15] AVG. loss: 0.998\n",
            "[10\t20] AVG. loss: 0.972\n",
            "[83.98, 81.47, 80.13, 76.89, 74.45, 69.08, 73.57, 66.74, 48.75, 89.89]\n",
            "[83.98 81.47 80.13 76.89 74.45 69.08 73.57 66.74 48.75 89.89]\n",
            "Average accuracy after training task 10: 74 %\n",
            "Use GPU\n",
            "Penalty term coefficient:  100\n",
            "Task\tEpoch\n",
            "[1\t5] AVG. loss: 2.091\n",
            "[1\t10] AVG. loss: 1.024\n",
            "[1\t15] AVG. loss: 0.561\n",
            "[1\t20] AVG. loss: 0.442\n",
            "[89.19]\n",
            "[89.19]\n",
            "Average accuracy after training task 1: 89 %\n",
            "[2\t5] AVG. loss: 0.810\n",
            "[2\t10] AVG. loss: 0.488\n",
            "[2\t15] AVG. loss: 0.432\n",
            "[2\t20] AVG. loss: 0.403\n",
            "[87.65, 90.24]\n",
            "[87.65 90.24]\n",
            "Average accuracy after training task 2: 88 %\n",
            "[3\t5] AVG. loss: 0.802\n",
            "[3\t10] AVG. loss: 0.523\n",
            "[3\t15] AVG. loss: 0.473\n",
            "[3\t20] AVG. loss: 0.446\n",
            "[84.67, 87.48, 90.51]\n",
            "[84.67 87.48 90.51]\n",
            "Average accuracy after training task 3: 87 %\n",
            "[4\t5] AVG. loss: 0.886\n",
            "[4\t10] AVG. loss: 0.619\n",
            "[4\t15] AVG. loss: 0.566\n",
            "[4\t20] AVG. loss: 0.536\n",
            "[86.78, 86.58, 85.11, 90.3]\n",
            "[86.78 86.58 85.11 90.3 ]\n",
            "Average accuracy after training task 4: 87 %\n",
            "[5\t5] AVG. loss: 1.033\n",
            "[5\t10] AVG. loss: 0.740\n",
            "[5\t15] AVG. loss: 0.683\n",
            "[5\t20] AVG. loss: 0.652\n",
            "[86.8, 87.63, 81.53, 75.98, 90.08]\n",
            "[86.8  87.63 81.53 75.98 90.08]\n",
            "Average accuracy after training task 5: 84 %\n",
            "[6\t5] AVG. loss: 1.156\n",
            "[6\t10] AVG. loss: 0.872\n",
            "[6\t15] AVG. loss: 0.818\n",
            "[6\t20] AVG. loss: 0.790\n",
            "[86.38, 86.84, 84.93, 82.0, 66.22, 89.6]\n",
            "[86.38 86.84 84.93 82.   66.22 89.6 ]\n",
            "Average accuracy after training task 6: 82 %\n",
            "[7\t5] AVG. loss: 1.164\n",
            "[7\t10] AVG. loss: 0.890\n",
            "[7\t15] AVG. loss: 0.841\n",
            "[7\t20] AVG. loss: 0.816\n",
            "[86.27, 87.65, 83.61, 82.8, 79.88, 57.92, 89.41]\n",
            "[86.27 87.65 83.61 82.8  79.88 57.92 89.41]\n",
            "Average accuracy after training task 7: 81 %\n",
            "[8\t5] AVG. loss: 1.442\n",
            "[8\t10] AVG. loss: 1.141\n",
            "[8\t15] AVG. loss: 1.090\n",
            "[8\t20] AVG. loss: 1.065\n",
            "[86.31, 86.27, 84.69, 81.8, 79.86, 73.96, 55.96, 87.77]\n",
            "[86.31 86.27 84.69 81.8  79.86 73.96 55.96 87.77]\n",
            "Average accuracy after training task 8: 79 %\n",
            "[9\t5] AVG. loss: 1.684\n",
            "[9\t10] AVG. loss: 1.365\n",
            "[9\t15] AVG. loss: 1.316\n",
            "[9\t20] AVG. loss: 1.292\n",
            "[85.26, 84.69, 82.98, 78.84, 80.73, 72.99, 71.88, 45.95, 87.78]\n",
            "[85.26 84.69 82.98 78.84 80.73 72.99 71.88 45.95 87.78]\n",
            "Average accuracy after training task 9: 76 %\n",
            "[10\t5] AVG. loss: 2.006\n",
            "[10\t10] AVG. loss: 1.705\n",
            "[10\t15] AVG. loss: 1.664\n",
            "[10\t20] AVG. loss: 1.645\n",
            "[86.01, 82.61, 80.73, 77.58, 76.67, 71.1, 69.63, 63.94, 33.2, 86.98]\n",
            "[86.01 82.61 80.73 77.58 76.67 71.1  69.63 63.94 33.2  86.98]\n",
            "Average accuracy after training task 10: 72 %\n",
            "Use GPU\n",
            "Penalty term coefficient:  300\n",
            "Task\tEpoch\n",
            "[1\t5] AVG. loss: 2.111\n",
            "[1\t10] AVG. loss: 1.076\n",
            "[1\t15] AVG. loss: 0.567\n",
            "[1\t20] AVG. loss: 0.440\n",
            "[89.22]\n",
            "[89.22]\n",
            "Average accuracy after training task 1: 89 %\n",
            "[2\t5] AVG. loss: 0.846\n",
            "[2\t10] AVG. loss: 0.526\n",
            "[2\t15] AVG. loss: 0.467\n",
            "[2\t20] AVG. loss: 0.435\n",
            "[88.05, 89.91]\n",
            "[88.05 89.91]\n",
            "Average accuracy after training task 2: 88 %\n",
            "[3\t5] AVG. loss: 0.886\n",
            "[3\t10] AVG. loss: 0.609\n",
            "[3\t15] AVG. loss: 0.554\n",
            "[3\t20] AVG. loss: 0.522\n",
            "[85.55, 87.58, 90.02]\n",
            "[85.55 87.58 90.02]\n",
            "Average accuracy after training task 3: 87 %\n",
            "[4\t5] AVG. loss: 1.019\n",
            "[4\t10] AVG. loss: 0.752\n",
            "[4\t15] AVG. loss: 0.697\n",
            "[4\t20] AVG. loss: 0.666\n",
            "[87.52, 88.14, 84.31, 89.04]\n",
            "[87.52 88.14 84.31 89.04]\n",
            "Average accuracy after training task 4: 87 %\n",
            "[5\t5] AVG. loss: 1.257\n",
            "[5\t10] AVG. loss: 0.946\n",
            "[5\t15] AVG. loss: 0.886\n",
            "[5\t20] AVG. loss: 0.853\n",
            "[87.48, 87.54, 80.98, 76.56, 88.75]\n",
            "[87.48 87.54 80.98 76.56 88.75]\n",
            "Average accuracy after training task 5: 84 %\n",
            "[6\t5] AVG. loss: 1.503\n",
            "[6\t10] AVG. loss: 1.203\n",
            "[6\t15] AVG. loss: 1.149\n",
            "[6\t20] AVG. loss: 1.120\n",
            "[86.94, 87.15, 83.55, 82.51, 63.79, 87.38]\n",
            "[86.94 87.15 83.55 82.51 63.79 87.38]\n",
            "Average accuracy after training task 6: 81 %\n",
            "[7\t5] AVG. loss: 1.579\n",
            "[7\t10] AVG. loss: 1.300\n",
            "[7\t15] AVG. loss: 1.255\n",
            "[7\t20] AVG. loss: 1.230\n",
            "[86.55, 87.54, 82.87, 81.08, 76.76, 57.7, 86.77]\n",
            "[86.55 87.54 82.87 81.08 76.76 57.7  86.77]\n",
            "Average accuracy after training task 7: 79 %\n",
            "[8\t5] AVG. loss: 1.979\n",
            "[8\t10] AVG. loss: 1.677\n",
            "[8\t15] AVG. loss: 1.632\n",
            "[8\t20] AVG. loss: 1.608\n",
            "[87.02, 87.23, 83.42, 81.66, 78.85, 75.53, 51.0, 84.44]\n",
            "[87.02 87.23 83.42 81.66 78.85 75.53 51.   84.44]\n",
            "Average accuracy after training task 8: 78 %\n",
            "[9\t5] AVG. loss: 2.449\n",
            "[9\t10] AVG. loss: 2.143\n",
            "[9\t15] AVG. loss: 2.104\n",
            "[9\t20] AVG. loss: 2.083\n",
            "[85.97, 84.85, 81.25, 78.71, 78.78, 69.89, 70.71, 44.25, 82.64]\n",
            "[85.97 84.85 81.25 78.71 78.78 69.89 70.71 44.25 82.64]\n",
            "Average accuracy after training task 9: 75 %\n",
            "[10\t5] AVG. loss: 3.117\n",
            "[10\t10] AVG. loss: 2.847\n",
            "[10\t15] AVG. loss: 2.820\n",
            "[10\t20] AVG. loss: 2.805\n",
            "[85.65, 82.52, 78.8, 78.35, 74.26, 72.96, 64.72, 60.7, 30.62, 80.14]\n",
            "[85.65 82.52 78.8  78.35 74.26 72.96 64.72 60.7  30.62 80.14]\n",
            "Average accuracy after training task 10: 70 %\n",
            "Use GPU\n",
            "Penalty term coefficient:  1000\n",
            "Task\tEpoch\n",
            "[1\t5] AVG. loss: 2.101\n",
            "[1\t10] AVG. loss: 1.054\n",
            "[1\t15] AVG. loss: 0.566\n",
            "[1\t20] AVG. loss: 0.443\n",
            "[89.12]\n",
            "[89.12]\n",
            "Average accuracy after training task 1: 89 %\n",
            "[2\t5] AVG. loss: 0.900\n",
            "[2\t10] AVG. loss: 0.585\n",
            "[2\t15] AVG. loss: 0.518\n",
            "[2\t20] AVG. loss: 0.480\n",
            "[88.39, 89.48]\n",
            "[88.39 89.48]\n",
            "Average accuracy after training task 2: 88 %\n",
            "[3\t5] AVG. loss: 1.059\n",
            "[3\t10] AVG. loss: 0.754\n",
            "[3\t15] AVG. loss: 0.681\n",
            "[3\t20] AVG. loss: 0.640\n",
            "[87.47, 87.62, 88.66]\n",
            "[87.47 87.62 88.66]\n",
            "Average accuracy after training task 3: 87 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWeAfeME6t8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}