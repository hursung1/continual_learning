{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_Continual_Learning_Final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hursung1/grad_project/blob/master/MNIST_Continual_Learning_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxSnkStJoYDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXtTo6L2TeQG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PermutedMNISTDataLoader(torchvision.datasets.MNIST):\n",
        "    \n",
        "    def __init__(self, source='./mnist_data', train = True, shuffle_seed = None):\n",
        "        super(PermutedMNISTDataLoader, self).__init__(source, train, download=True)\n",
        "        \n",
        "        self.train = train\n",
        "        if self.train:\n",
        "            self.permuted_train_data = torch.stack(\n",
        "                [img.type(dtype=torch.float32).view(-1)[shuffle_seed] / 255.0\n",
        "                    for img in self.train_data])\n",
        "        else:\n",
        "            self.permuted_test_data = torch.stack(\n",
        "                [img.type(dtype=torch.float32).view(-1)[shuffle_seed] / 255.0\n",
        "                    for img in self.test_data])\n",
        "            \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        if self.train:\n",
        "            input, label = self.permuted_train_data[index], self.train_labels[index]\n",
        "        else:\n",
        "            input, label = self.permuted_test_data[index], self.test_labels[index]\n",
        "        \n",
        "        return input, label\n",
        "\n",
        "    def sample(self, size):\n",
        "        return [img for img in self.permuted_train_data[random.sample(range(len(self), size))]]\n",
        "    \n",
        "    '''\n",
        "    def __len__(self):\n",
        "        if self.train:\n",
        "            return self.train_data.size()\n",
        "        else:\n",
        "            return self.test_data.size()\n",
        "    '''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEUfd3K_oiYV",
        "colab_type": "code",
        "outputId": "b050d16f-570d-4699-8940-79c21be0a307",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "batch_size = 64\n",
        "num_tasks = 10\n",
        "\n",
        "def permute_mnist():\n",
        "    train_loader = {}\n",
        "    test_loader = {}\n",
        "    \n",
        "    for i in range(num_tasks):\n",
        "        shuffle_seed = np.arange(28*28)\n",
        "        np.random.shuffle(shuffle_seed)\n",
        "        train_loader[i] = torch.utils.data.DataLoader(\n",
        "            PermutedMNISTDataLoader(train=True, shuffle_seed=shuffle_seed),\n",
        "                batch_size=batch_size)\n",
        "        \n",
        "        test_loader[i] = torch.utils.data.DataLoader(\n",
        "            PermutedMNISTDataLoader(train=False, shuffle_seed=shuffle_seed),\n",
        "                batch_size=batch_size)\n",
        "    \n",
        "    return train_loader, test_loader\n",
        "\n",
        "train_loader, test_loader = permute_mnist()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:53: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:58: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqYx7HoWon7d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "cb713fc1-347a-45cd-f5b7-44c390ee5b1d"
      },
      "source": [
        "'''\n",
        "batch_size = 64\n",
        "\n",
        "#DataLoader: read batsh_size number of data from dataset \n",
        "train_loader = torch.utils.data.DataLoader(train_set, \n",
        "                                           batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_set,\n",
        "                                        batch_size=batch_size, shuffle=False)\n",
        "'''                                        "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nbatch_size = 64\\n\\n#DataLoader: read batsh_size number of data from dataset \\ntrain_loader = torch.utils.data.DataLoader(train_set, \\n                                           batch_size=batch_size, shuffle=True)\\ntest_loader = torch.utils.data.DataLoader(test_set,\\n                                        batch_size=batch_size, shuffle=False)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M6_A9nTJYDi",
        "colab_type": "text"
      },
      "source": [
        "### Define Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfTLE0m3opep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        # Always start with inheriting torch.nn.Module\n",
        "        # Ancestor class of all Neural Net module\n",
        "        super(NeuralNet, self).__init__()\n",
        "        # Linear: linear transformation\n",
        "        fc1 = nn.Linear(28*28, 400)\n",
        "        fc2 = nn.Linear(400, 400)\n",
        "        fc3 = nn.Linear(400, 10)\n",
        "  \n",
        "        \n",
        "        self.fc_module = nn.Sequential(\n",
        "            fc1,\n",
        "            nn.ReLU(),\n",
        "            fc2,\n",
        "            nn.ReLU(),\n",
        "            fc3\n",
        "        )\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            self.fc_module = self.fc_module.cuda()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc_module(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkX7OCUYJO5R",
        "colab_type": "text"
      },
      "source": [
        "### Get Fisher Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7OWWTJV3KpZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fisher(net, data_loader, task):\n",
        "    fisher_mat = []\n",
        "    #start_time = time.time()       \n",
        "    for i in range(task):\n",
        "        #data = train_loader[task].dataset.__getitem__(task - i)[0]\n",
        "        for n, data in enumerate(data_loader[i]):\n",
        "            data_ = data[0]\n",
        "            break\n",
        "\n",
        "        #print(data.size())\n",
        "        if torch.cuda.is_available():\n",
        "            data_ = data_.cuda()\n",
        "\n",
        "        params = {n : p for n, p in net.named_parameters() if p.requires_grad}\n",
        "        fisher_mat_per_task = {}\n",
        "        for n, p in deepcopy(params).items():\n",
        "            p.data.zero_()\n",
        "            fisher_mat_per_task[n] = p.data\n",
        "\n",
        "        net.eval()\n",
        "        for data in data_:\n",
        "            net.zero_grad()\n",
        "            output = net(data).view(1, -1)\n",
        "            pred = output.max(1)[1].view(-1)\n",
        "            loss = F.nll_loss(F.log_softmax(output, dim=1), pred)\n",
        "            loss.backward()\n",
        "\n",
        "            for n, p in net.named_parameters():\n",
        "                fisher_mat_per_task[n].data += p.grad.data ** 2 / len(data_)\n",
        "\n",
        "        fisher_mat.append({n : p for n, p in fisher_mat_per_task.items()})\n",
        "        #print(\"Time: %.3f\" %(time.time() - start_time))\n",
        "    return fisher_mat\n",
        "\n",
        "############################################################################                \n",
        "    '''\n",
        "    params = {n : p for n, p in net.named_parameters() if p.requires_grad}\n",
        "    fisher_mat = {}\n",
        "    for n, p in deepcopy(params).items():\n",
        "        p.data.zero_()\n",
        "        fisher_mat[n] = p.data\n",
        "\n",
        "    net.eval()\n",
        "    for data in input:\n",
        "        net.zero_grad()\n",
        "        output = net(data).view(1, -1)\n",
        "        pred = output.max(1)[1].view(-1)\n",
        "        loss = F.nll_loss(F.log_softmax(output, dim=1), pred)\n",
        "        loss.backward()\n",
        "\n",
        "        for n, p in net.named_parameters():\n",
        "            fisher_mat[n].data += p.grad.data ** 2 / len(input)\n",
        "\n",
        "    fisher_mat = {n : p for n, p in fisher_mat.items()}\n",
        "    #print(\"Time: %.3f\" %(time.time() - start_time))\n",
        "    return fisher_mat\n",
        "    '''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tilDKYw6JT8v",
        "colab_type": "text"
      },
      "source": [
        "### Learning Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dV5puTlos0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Continual_Learning(net, optimizer, num_tasks, reg_coef, learn_mode = 0):\n",
        "    if learn_mode > 2 or learn_mode < 0:\n",
        "        print(\"Learn mode Error\\nplain: 0\\tpenalty with L2 distance: 1\\tpenalty with ewc: 2\")\n",
        "        return False\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    num_epochs = 20\n",
        "    sample_size = 100\n",
        "    acc = {}\n",
        "    params_per_tasks = []\n",
        "\n",
        "    print(\"Task\\tEpoch\")\n",
        "    for task in range(num_tasks):\n",
        "        running_loss = 0.0\n",
        "        \n",
        "        # Get Fisher Matrix\n",
        "        if len(params_per_tasks) != 0 and learn_mode == 2:\n",
        "            fisher_mat = fisher(net, train_loader, task)\n",
        "            '''\n",
        "            for j in range(task):\n",
        "                for i, data in enumerate(train_loader[task]):\n",
        "                    inputs = data[0]\n",
        "                    if torch.cuda.is_available():\n",
        "                        inputs = inputs.cuda()\n",
        "                    #print(inputs.size())\n",
        "                    fisher_mat.append(fisher(net, inputs))\n",
        "                    break\n",
        "            '''\n",
        "        # Train for each task\n",
        "        for epoch in range(num_epochs):\n",
        "            for i, data in enumerate(train_loader[task]):\n",
        "                inputs, labels = data\n",
        "\n",
        "                if torch.cuda.is_available():\n",
        "                    inputs = inputs.cuda()\n",
        "                    labels = labels.cuda()\n",
        "\n",
        "                # gradient initiallize\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Compute forward-propagation\n",
        "                outputs = net(inputs)\n",
        "\n",
        "                # Compute Loss\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Compute Loss & L2 distance\n",
        "                if learn_mode != 0:\n",
        "                    reg = 0\n",
        "                    ind = 0\n",
        "                    for params_past in params_per_tasks:\n",
        "                        for n, p in net.named_parameters():\n",
        "                            if torch.cuda.is_available():\n",
        "                                params_past[n] = params_past[n].cuda()\n",
        "                                \n",
        "                            penalty = (params_past[n] - p)**2\n",
        "                            # EWC: multiply fisher matrix\n",
        "                            if learn_mode == 2:\n",
        "                                penalty = fisher_mat[ind][n] * penalty\n",
        "\n",
        "                            reg += torch.sum(penalty)\n",
        "                        ind += 1\n",
        "                    loss = loss + (reg_coef / 2) * reg\n",
        "\n",
        "                #Do Back-propagation\n",
        "                loss.backward()\n",
        "                #Weight update\n",
        "                optimizer.step()\n",
        "\n",
        "                #cumulate loss\n",
        "                running_loss += loss.data.item()\n",
        "\n",
        "            if epoch % 5 == 4:\n",
        "                print('[%d\\t%d] AVG. loss: %.3f' % (task+1, epoch + 1, running_loss/(i*5)))\n",
        "                running_loss = 0.0\n",
        "\n",
        "        # Save parameters to use at next iteration: used to cal. penalty term      \n",
        "        if learn_mode != 0:\n",
        "            tp = {n : p for n, p in net.named_parameters() if p.requires_grad}\n",
        "            params = {}\n",
        "            for n, p in deepcopy(tp).items():\n",
        "                params[n] = p.data\n",
        "            params_per_tasks.append(params)\n",
        "\n",
        "            \n",
        "        # Test for each task after learning a task.\n",
        "        each_task_acc = []\n",
        "        for j in range(task+1):\n",
        "            total = 0\n",
        "            correct = 0\n",
        "            for i, data in enumerate(test_loader[j]):\n",
        "                inputs, labels = data\n",
        "                if torch.cuda.is_available():\n",
        "                    inputs = inputs.cuda()\n",
        "                    labels = labels.cuda()\n",
        "\n",
        "                # forward propagation\n",
        "                outputs = net(inputs)\n",
        "\n",
        "                # torch.max: returns maximum value of a tensor\n",
        "                _, predicted = torch.max(outputs.data, dim=1)\n",
        "                total += labels.size(0)\n",
        "                \n",
        "                # Estimate accuracy of model\n",
        "                correct += (predicted == labels).sum()\n",
        "                                \n",
        "            each_task_acc.append(100 * correct.cpu().numpy() / total)\n",
        "            \n",
        "        print(each_task_acc)\n",
        "        each_task_acc = np.asarray(each_task_acc)\n",
        "        print(each_task_acc)\n",
        "        print('Average accuracy after training task %d: %d %%' % (task+1, np.mean(each_task_acc)))\n",
        "        acc[task] = np.mean(each_task_acc)\n",
        "            # For each input data, print the accuracy of the model\n",
        "            #print('Accuracy of the network on the test images %d for task %d after training task %d: %d %%' \n",
        "            #                % (i, j, task+1, 100 * correct / total))\n",
        "            \n",
        "    return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1oVDzZEqL92",
        "colab_type": "code",
        "outputId": "b94d69e2-b7d0-4dc7-f9c6-f185037e292b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        }
      },
      "source": [
        "learning_rate = 1e-3\n",
        "num_task = 10\n",
        "\n",
        "reg_coef_list = []\n",
        "learning_acc = {}\n",
        "\n",
        "# Learning mode set\n",
        "#   if 0: plain SGD\n",
        "#   if 1: penalty term - L2 distance\n",
        "#   if 2: penalty term - EWC\n",
        "#   else: Error \n",
        "learn_mode = 2\n",
        "if learn_mode == 0:\n",
        "    print(\"Learn Mode: Plain SGD\")\n",
        "    reg_coef_list.append(1)\n",
        "elif learn_mode == 1:\n",
        "    print(\"Learn Mode: Penalty term with L2 distance\")\n",
        "    for i in range(1, 11):\n",
        "        reg_coef_list.append(i * 0.001)\n",
        "elif learn_mode == 2:\n",
        "    print(\"Learn Mode: Penalty term with EWC\")\n",
        "    for i in range(1, 11):\n",
        "        reg_coef_list.append(i * 0.001)\n",
        "else:\n",
        "    print(\"Wrong value\")\n",
        "\n",
        "for reg_coef in reg_coef_list:\n",
        "    net = NeuralNet()\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"Use GPU\")\n",
        "        net.cuda()\n",
        "        \n",
        "    optimizer = optim.SGD(net.parameters(), lr=learning_rate)    \n",
        "    print(\"Penalty term coefficient: \", reg_coef)\n",
        "    learning_acc[reg_coef] = Continual_Learning(net, optimizer, num_task, reg_coef, learn_mode=learn_mode)\n",
        "\n",
        "if(learning_acc):\n",
        "    x = []\n",
        "    y = []\n",
        "    for key, value in learning_acc.items():\n",
        "        x.append(key)\n",
        "        y.append(value)\n",
        "    plt.plot(x, y)\n",
        "    plt.show()\n",
        "else:\n",
        "    print('헛짓거리함')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learn Mode: Penalty term with EWC\n",
            "Use GPU\n",
            "Penalty term coefficient:  0.001\n",
            "Task\tEpoch\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:43: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1\t5] AVG. loss: 2.124\n",
            "[1\t10] AVG. loss: 1.111\n",
            "[1\t15] AVG. loss: 0.578\n",
            "[1\t20] AVG. loss: 0.445\n",
            "[89.11]\n",
            "[89.11]\n",
            "Average accuracy after training task 1: 89 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:48: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2\t5] AVG. loss: 0.804\n",
            "[2\t10] AVG. loss: 0.437\n",
            "[2\t15] AVG. loss: 0.375\n",
            "[2\t20] AVG. loss: 0.342\n",
            "[84.37, 90.98]\n",
            "[84.37 90.98]\n",
            "Average accuracy after training task 2: 87 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-5bc08020b090>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Penalty term coefficient: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_coef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mlearning_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreg_coef\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mContinual_Learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_task\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_coef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearn_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-c9a6e08a005f>\u001b[0m in \u001b[0;36mContinual_Learning\u001b[0;34m(net, optimizer, num_tasks, reg_coef, learn_mode)\u001b[0m\n\u001b[1;32m     59\u001b[0m                                 \u001b[0mpenalty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfisher_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                             \u001b[0mreg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                         \u001b[0mind\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreg_coef\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieHiuIDDo_Jf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_mode = 2 \n",
        "if learn_mode == 0:\n",
        "    print(\"Learn Mode: Plain SGD\")\n",
        "    reg_coef_list.append(1)\n",
        "elif learn_mode == 1:\n",
        "    print(\"Learn Mode: Penalty term with L2 distance\")\n",
        "    for i in range(1, 11):\n",
        "        reg_coef_list.append(i * 0.001)\n",
        "elif learn_mode == 2:\n",
        "    print(\"Learn Mode: Penalty term with EWC\")\n",
        "    for i in range(1, 11):\n",
        "        reg_coef_list.append(i * 0.001)\n",
        "else:\n",
        "    print(\"Wrong value\")\n",
        "\n",
        "for reg_coef in reg_coef_list:\n",
        "    net = NeuralNet()\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"Use GPU\")\n",
        "        net.cuda()\n",
        "        \n",
        "    optimizer = optim.SGD(net.parameters(), lr=learning_rate)    \n",
        "    print(\"Penalty term coefficient: \", reg_coef)\n",
        "    learning_acc[reg_coef] = Continual_Learning(net, optimizer, num_task, reg_coef, learn_mode=learn_mode)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb-0sgFDbQPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc = learning_acc[1]\n",
        "print(acc)\n",
        "x = []\n",
        "y = []\n",
        "for key, value in acc.items():\n",
        "    x.append(key)\n",
        "    y.append(value)\n",
        "plt.plot(x, y)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}